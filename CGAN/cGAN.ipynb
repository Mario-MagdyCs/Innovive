{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faPhvaNwU4dk",
        "outputId": "a2a7d391-d079-4498-df23-3be1a195d14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Define materials of interest and folders to store images and project names\n",
        "materials = [\"Plastic Bottle\", \"Fabric\", \"Glass Bottle\"]\n",
        "base_dir = \"/content/drive/MyDrive/Material_Project_Data\"\n",
        "\n",
        "# Create main directory if it doesn't exist\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Load JSON objects\n",
        "json_file_path = \"/content/drive/MyDrive/DIY_DATASET.json\"  # Replace with your actual JSON file path\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Function to create folders for each material\n",
        "def create_folders(material):\n",
        "    image_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Images\")\n",
        "    name_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Names\")\n",
        "    os.makedirs(image_folder, exist_ok=True)\n",
        "    os.makedirs(name_folder, exist_ok=True)\n",
        "    return image_folder, name_folder\n",
        "\n",
        "# Function to download and save an image\n",
        "def download_image(url, save_path):\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            with open(save_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(1024):\n",
        "                    f.write(chunk)\n",
        "        else:\n",
        "            print(f\"Failed to download image from {url}: HTTP {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading image from {url}: {e}\")\n",
        "\n",
        "# Iterate through each material\n",
        "for material in materials:\n",
        "    image_folder, name_folder = create_folders(material)\n",
        "\n",
        "    # Iterate through each project in the JSON data\n",
        "    for project in data:\n",
        "        project_name = project.get(\"Product_Name\", \"Unnamed_Project\")\n",
        "        image_url = project.get(\"Image_Url\", \"\")\n",
        "        project_materials = project.get(\"Materials\", [])\n",
        "\n",
        "        # Check if the material is part of the project materials\n",
        "        if any(material.lower() in mat.lower() for mat in project_materials):\n",
        "            # Format the project name to include the material\n",
        "            formatted_name = f\"{material.replace(' ', '_')}_{project_name.replace(' ', '_')}\"\n",
        "\n",
        "            # Save project name as a text file\n",
        "            project_name_file = os.path.join(name_folder, f\"{formatted_name}.txt\")\n",
        "            with open(project_name_file, 'w') as f:\n",
        "                f.write(f\"Project Name: {project_name}\\n\")\n",
        "                f.write(f\"Webpage URL: {project.get('Webpage_Url', 'N/A')}\\n\")\n",
        "                f.write(f\"Category: {project.get('Category', 'N/A')}\\n\")\n",
        "                f.write(f\"Primary Function: {project.get('Primary_Function', 'N/A')}\\n\")\n",
        "                f.write(f\"Materials: {', '.join(project_materials)}\\n\")\n",
        "                f.write(f\"Difficulty: {project.get('Difficulty', 'N/A')}\\n\")\n",
        "                f.write(\"\\nSteps:\\n\")\n",
        "                for step in project.get(\"Steps\", []):\n",
        "                    f.write(f\"Step {step['step']}: {step['description']}\\n\")\n",
        "\n",
        "            # Download and save the project image\n",
        "            if image_url:\n",
        "                image_file_path = os.path.join(image_folder, f\"{formatted_name}.jpg\")\n",
        "                download_image(image_url, image_file_path)\n"
      ],
      "metadata": {
        "id": "TrHPYR3LXbb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define materials and base directory\n",
        "materials = [\"Plastic Bottle\", \"Fabric\", \"Glass Bottle\"]\n",
        "base_dir = \"/content/drive/MyDrive/Material_Project_Data\"\n",
        "\n",
        "# Loop through each material\n",
        "for material in materials:\n",
        "    # Get the paths for image and project name folders\n",
        "    image_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Images\")\n",
        "    name_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Names\")\n",
        "\n",
        "    # Loop through each project name file\n",
        "    for project_name_file in os.listdir(name_folder):\n",
        "        # Get the corresponding image file path\n",
        "        project_base_name = os.path.splitext(project_name_file)[0]\n",
        "        corresponding_image_file = os.path.join(image_folder, f\"{project_base_name}.jpg\")\n",
        "\n",
        "        # Check if the image file exists\n",
        "        if not os.path.exists(corresponding_image_file):\n",
        "            # If the image is missing, remove the project name file\n",
        "            project_name_file_path = os.path.join(name_folder, project_name_file)\n",
        "            os.remove(project_name_file_path)\n",
        "            print(f\"Removed project name file: {project_name_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LgyQMmeZUVt",
        "outputId": "23fb350e-9500-44eb-b397-917926b48c84",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed project name file: Plastic_Bottle_Chalkboard_Fall_Vignette.txt\n",
            "Removed project name file: Plastic_Bottle_DIY_Gift_Box_for_Kids_(Recycled_Plastic_Bottle).txt\n",
            "Removed project name file: Plastic_Bottle_Gift_Wrapping_using_Disposable_Water_Bottles.txt\n",
            "Removed project name file: Plastic_Bottle_Plastic_Bottle_Gift_Box.txt\n",
            "Removed project name file: Fabric_Spray_Painted_Succulent_Pots.txt\n",
            "Removed project name file: Fabric_DIY_Photo_Display_Board.txt\n",
            "Removed project name file: Fabric_Yarn_Wrapped_Hearts_Valentine_Garland.txt\n",
            "Removed project name file: Fabric_Fabric_and_Wood_Bead_Sunflowers.txt\n",
            "Removed project name file: Fabric_DIY_Fabric_Flowers_No_Sew_Garland.txt\n",
            "Removed project name file: Fabric_Painted_Old_Picture_Frames.txt\n",
            "Removed project name file: Fabric_DIY_Rope_Flower_Pot.txt\n",
            "Removed project name file: Fabric_Vintage_Linens_Garland.txt\n",
            "Removed project name file: Fabric_Easy_Lavender_Sachets.txt\n",
            "Removed project name file: Fabric_Scrap_Fabric_Bowl.txt\n",
            "Removed project name file: Fabric_DIY_No_Sew_Pin_Cushion_Using_a_Pine_Cone.txt\n",
            "Removed project name file: Fabric_Fabric_Scrap_Patchwork.txt\n",
            "Removed project name file: Fabric_Sewn_Fabric_Bookmarks.txt\n",
            "Removed project name file: Fabric_DIY_Fabric_Covered_Flower_Pots.txt\n",
            "Removed project name file: Fabric_Ruffled_Chair_Skirt.txt\n",
            "Removed project name file: Fabric_DIY_Denim_Hanging_Jar_Vase_With_Paper_Daisies.txt\n",
            "Removed project name file: Fabric_DIY_Wine_Bottle_Tiki_Torches_and_Solar_Lights.txt\n",
            "Removed project name file: Fabric_Colored_Wine_Bottles.txt\n",
            "Removed project name file: Fabric_Altered_Bottle.txt\n",
            "Removed project name file: Fabric_Fabric_Bows.txt\n",
            "Removed project name file: Fabric_DIY_Fabric_Rose.txt\n",
            "Removed project name file: Fabric_Upcycled_Gift_Wrap_-_Cardboard_Box_&_Old_Clothes.txt\n",
            "Removed project name file: Fabric_Spring_Vase_from_Recycled_Tin_Can.txt\n",
            "Removed project name file: Fabric_Upcycled_Tin_Can_Craft_Storage_with_Cricut.txt\n",
            "Removed project name file: Fabric_Tin_Can_Pencil_Holder.txt\n",
            "Removed project name file: Fabric_Fabric_Covered_Flower_Containers.txt\n",
            "Removed project name file: Fabric_Patriotic_Repurposed_Can_Birdfeeder.txt\n",
            "Removed project name file: Fabric_Appliqué_Coasters.txt\n",
            "Removed project name file: Fabric_Cone_Potholder.txt\n",
            "Removed project name file: Fabric_Quilted_Trivet.txt\n",
            "Removed project name file: Fabric_5_Minute_Catnip_Toy.txt\n",
            "Removed project name file: Fabric_Bunny_Ears_Headband.txt\n",
            "Removed project name file: Fabric_Reusable_Makeup_Remover_Wipes.txt\n",
            "Removed project name file: Fabric_Lucky_Chicken_Pattern.txt\n",
            "Removed project name file: Fabric_Simple_Knotted_Headband.txt\n",
            "Removed project name file: Fabric_Soothing_Eye_Mask.txt\n",
            "Removed project name file: Fabric_Quilt_Piece_Bunting.txt\n",
            "Removed project name file: Fabric_Monkey_Key_Pouch.txt\n",
            "Removed project name file: Fabric_Festive_Napkin_Rings.txt\n",
            "Removed project name file: Fabric_Boston_Terrier_Puppy_Coin_Purse.txt\n",
            "Removed project name file: Fabric_Mini_Foldable_Baskets.txt\n",
            "Removed project name file: Fabric_YoYo_Stuffed_Dog_Sewing_Pattern.txt\n",
            "Removed project name file: Fabric_Pixie_Cup.txt\n",
            "Removed project name file: Fabric_Small_Zipper_Pouch.txt\n",
            "Removed project name file: Fabric_Handmade_Quilted_Postcard.txt\n",
            "Removed project name file: Fabric_Decorative_Tape_from_Fabric_Scraps.txt\n",
            "Removed project name file: Fabric_Face_Mask_Sewing_Pattern.txt\n",
            "Removed project name file: Fabric_Handy_Dandy_Pouch_-_Skinny_Drawstring_Bag.txt\n",
            "Removed project name file: Fabric_DIY_Stand_Up_Pencil_Pouch.txt\n",
            "Removed project name file: Fabric_Tree_Quilt_Block.txt\n",
            "Removed project name file: Fabric_Doll_Clothes_from_Misshapen_Fabric_Scraps.txt\n",
            "Removed project name file: Fabric_Pinwheel_Quilt_Block.txt\n",
            "Removed project name file: Fabric_Liberty_Fabric_Button_Push_Pin_Art.txt\n",
            "Removed project name file: Fabric_Elephant_Stuffed_Animal.txt\n",
            "Removed project name file: Fabric_Macaron_Coin_Purse.txt\n",
            "Removed project name file: Fabric_Bunny_Sachet_Bag.txt\n",
            "Removed project name file: Fabric_Simple_Heart_Sachet.txt\n",
            "Removed project name file: Fabric_Decoupage_Vase_with_Fabric.txt\n",
            "Removed project name file: Fabric_Braided_Rag_Rug.txt\n",
            "Removed project name file: Fabric_Mug_Rug_from_Upcycled_Denim.txt\n",
            "Removed project name file: Fabric_Turtle_Pincushion.txt\n",
            "Removed project name file: Fabric_Handmade_Quilted_Christmas_Ornaments.txt\n",
            "Removed project name file: Fabric_Triangle_Fold_Coaster.txt\n",
            "Removed project name file: Fabric_Luxury_Handmade_Gift_Tags_From_Scraps.txt\n",
            "Removed project name file: Fabric_Hexagon_Quilting_with_English_Paper_Piecing.txt\n",
            "Removed project name file: Fabric_DIY_Fabric_Book_Cover.txt\n",
            "Removed project name file: Fabric_Happy_Cats_Sewing_Pattern.txt\n",
            "Removed project name file: Fabric_Little_Chicks_Sewing_Pattern.txt\n",
            "Removed project name file: Fabric_DIY_Cord_Keepers.txt\n",
            "Removed project name file: Fabric_DIY_Fabric_Scrap_Flags.txt\n",
            "Removed project name file: Fabric_Fabric_Scraps_Garland.txt\n",
            "Removed project name file: Fabric_Cool_Embroidered_Scrap_Fabric_Cuff_Bracelets.txt\n",
            "Removed project name file: Fabric_Fishy_Friends_Stuffed_Animal_Pattern.txt\n",
            "Removed project name file: Fabric_Fabric_Postage_Stamps.txt\n",
            "Removed project name file: Fabric_Fabric_Moth_and_Butterfly_Sewing_Pattern.txt\n",
            "Removed project name file: Fabric_Fabric_Scrap_Shoelaces.txt\n",
            "Removed project name file: Fabric_Fabric_Butterflies.txt\n",
            "Removed project name file: Fabric_Seaglass_Quilting.txt\n",
            "Removed project name file: Fabric_House_Fly_Rag_Doll.txt\n",
            "Removed project name file: Fabric_Wire_Doll_Bed_DIY.txt\n",
            "Removed project name file: Fabric_Fabric_Flower_Making_Tutorial.txt\n",
            "Removed project name file: Fabric_Handmade_Scrunchies.txt\n",
            "Removed project name file: Glass_Bottle_Magical_Mason_Jar_Oil_Lamp.txt\n",
            "Removed project name file: Glass_Bottle_Simple_Bottle_Decoration_with_Chalkboard_Labels.txt\n",
            "Removed project name file: Glass_Bottle_Frosted_Wine_Bottle_Centerpiece.txt\n",
            "Removed project name file: Glass_Bottle_Faux_Bottle_Painting_using_Cricut.txt\n",
            "Removed project name file: Glass_Bottle_Personalized_Wine_Bottle_Picture_Holders.txt\n",
            "Removed project name file: Glass_Bottle_DIY_Wine_Bottle_Tiki_Torches_and_Solar_Lights.txt\n",
            "Removed project name file: Glass_Bottle_Wine_Bottle_Lamp.txt\n",
            "Removed project name file: Glass_Bottle_DIY_Macrame_Wine_Bottle_Hanger.txt\n",
            "Removed project name file: Glass_Bottle_Wine_Bottle_Bird_Feeder.txt\n",
            "Removed project name file: Glass_Bottle_Colored_Wine_Bottles.txt\n",
            "Removed project name file: Glass_Bottle_Altered_Bottle.txt\n",
            "Removed project name file: Glass_Bottle_Sunflower_Wine_Bottle_Centerpiece.txt\n",
            "Removed project name file: Glass_Bottle_Bunny_Sachet_Bag.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "dataset_folder = '/content/drive/MyDrive/Images With Captions'\n",
        "organized_folder = os.path.join(dataset_folder, 'organized_dataset')\n",
        "\n",
        "# Create the organized_dataset folder if it doesn't exist\n",
        "if not os.path.exists(organized_folder):\n",
        "    os.makedirs(organized_folder)\n",
        "\n",
        "# Iterate over each material folder\n",
        "for material_folder in os.listdir(dataset_folder):\n",
        "    material_path = os.path.join(dataset_folder, material_folder)\n",
        "\n",
        "    if os.path.isdir(material_path):\n",
        "        # Create a new material folder in the organized dataset\n",
        "        new_material_path = os.path.join(organized_folder, material_folder)\n",
        "        if not os.path.exists(new_material_path):\n",
        "            os.makedirs(new_material_path)\n",
        "\n",
        "        # Copy all image files into the new material folder\n",
        "        for image_file in os.listdir(material_path):\n",
        "            if image_file.endswith(('.jpg', '.png', '.jpeg')):  # Adjust extensions if necessary\n",
        "                shutil.copy(os.path.join(material_path, image_file), new_material_path)\n",
        "\n",
        "        # Find the corresponding caption file for this material\n",
        "        caption_file_name = f\"{material_folder.lower().replace(' ', '_')}_captions.txt\"\n",
        "        caption_file_path = os.path.join(dataset_folder, caption_file_name)\n",
        "\n",
        "        # Check if the caption file exists\n",
        "        if os.path.isfile(caption_file_path):\n",
        "            # Read all captions from the main caption file\n",
        "            with open(caption_file_path, 'r') as f:\n",
        "                captions = f.readlines()\n",
        "\n",
        "            # Sort images by file name (assuming names like '00001.jpg')\n",
        "            images = sorted(os.listdir(material_path))\n",
        "\n",
        "            # Create individual caption files for each image\n",
        "            for idx, image_file in enumerate(images):\n",
        "                if image_file.endswith(('.jpg', '.png', '.jpeg')):  # Ensure it's an image file\n",
        "                    image_name, _ = os.path.splitext(image_file)\n",
        "\n",
        "                    # Ensure there's a corresponding caption\n",
        "                    if idx < len(captions):\n",
        "                        caption = captions[idx].strip()\n",
        "\n",
        "                        # Create the individual caption file in the new material folder\n",
        "                        caption_output_path = os.path.join(new_material_path, f\"{image_name}.txt\")\n",
        "                        with open(caption_output_path, 'w') as f:\n",
        "                            f.write(caption)\n",
        "\n",
        "print(\"Organized dataset has been created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyYtIi4dL4aZ",
        "outputId": "7ca61417-df92-4ace-d8e8-18d04062d2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organized dataset has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import torchvision.utils as vutils\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfDI8VxAZkad",
        "outputId": "5f7c7c23-f0ea-4512-d803-662d76942810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import spacy\n",
        "\n",
        "# Load SpaCy for text embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")  # 300-dimensional embeddings\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "lr = 0.0002\n",
        "nz = 100  # Noise dimension\n",
        "num_epochs = 100\n",
        "image_size = 64  # Resize images to 64x64\n",
        "\n",
        "# Generator network\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, nz, ngf, nc, text_embedding_dim=300):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.label_embedding = nn.Linear(text_embedding_dim, nz)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz * 2, ngf * 16, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, text_embedding):\n",
        "        text_embedding = self.label_embedding(text_embedding)\n",
        "        text_embedding = text_embedding.unsqueeze(-1).unsqueeze(-1)\n",
        "        combined_input = torch.cat((noise, text_embedding), 1)\n",
        "        return self.main(combined_input)\n",
        "\n",
        "\n",
        "# Discriminator network\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, nc, ndf, text_embedding_dim=300):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.text_embedding_proj = nn.Linear(text_embedding_dim, ndf * 8)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 8 + ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text_embedding):\n",
        "        image_features = self.main(image)\n",
        "        text_features = self.text_embedding_proj(text_embedding)\n",
        "        text_features = text_features.unsqueeze(-1).unsqueeze(-1).expand_as(image_features)\n",
        "        combined_features = torch.cat((image_features, text_features), 1)\n",
        "        return self.final_layer(combined_features)\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "class ProjectDataset(Dataset):\n",
        "    def __init__(self, base_dir, materials, transform):\n",
        "        self.image_paths = []\n",
        "        self.texts = []\n",
        "\n",
        "        # Collect all images and their corresponding project descriptions\n",
        "        for material in materials:\n",
        "            image_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Images\")\n",
        "            text_folder = os.path.join(base_dir, f\"{material.replace(' ', '_')}_Project_Names\")\n",
        "\n",
        "            image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\")]\n",
        "            for image_file in image_files:\n",
        "                image_path = os.path.join(image_folder, image_file)\n",
        "                project_name = os.path.splitext(image_file)[0]\n",
        "                text_file = os.path.join(text_folder, f\"{project_name}.txt\")\n",
        "\n",
        "                if os.path.exists(text_file):\n",
        "                    self.image_paths.append(image_path)\n",
        "                    with open(text_file, 'r') as f:\n",
        "                        self.texts.append(f.read())\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load and convert image to RGB\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGBA\")  # Convert palette images to RGBA\n",
        "        image = image.convert(\"RGB\")  # Ensure all images are in RGB format\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Get text embedding\n",
        "        text_embedding = torch.tensor(nlp(self.texts[idx]).vector, dtype=torch.float32)\n",
        "\n",
        "        return image, text_embedding\n",
        "\n",
        "\n",
        "\n",
        "# Initialize dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "dataset = ProjectDataset(\n",
        "    base_dir=\"/content/drive/MyDrive/Material_Project_Data\",\n",
        "    materials=[\"Plastic Bottle\", \"Fabric\", \"Glass Bottle\"],\n",
        "    transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize models, optimizers, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = ConditionalGenerator(nz, 64, 3).to(device)\n",
        "discriminator = ConditionalDiscriminator(3, 64).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, text_embeddings) in enumerate(dataloader):\n",
        "        real_images = real_images.to(device)\n",
        "        text_embeddings = text_embeddings.to(device)\n",
        "\n",
        "        # Train discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        # Real images\n",
        "        real_labels = torch.ones(real_images.size(0), 1, 1, 1, device=device)\n",
        "        fake_labels = torch.zeros(real_images.size(0), 1, 1, 1, device=device)\n",
        "\n",
        "        # Forward pass with real images\n",
        "        outputs = discriminator(real_images, text_embeddings)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        # Fake images\n",
        "        noise = torch.randn(real_images.size(0), nz, 1, 1, device=device)\n",
        "        fake_images = generator(noise, text_embeddings)\n",
        "        outputs = discriminator(fake_images.detach(), text_embeddings)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "\n",
        "        # Backpropagate and update discriminator\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train generator\n",
        "        optimizer_g.zero_grad()\n",
        "        outputs = discriminator(fake_images, text_embeddings)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        # Backpropagate and update generator\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n",
        "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "        torch.save(generator.state_dict(), f\"/content/drive/MyDrive/Material_Project_Data/generator_epoch_{epoch+1}.pth\")\n",
        "        torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/Material_Project_Data/discriminator_epoch_{epoch+1}.pth\")\n",
        "\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "_5Jlu7AEMLWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}